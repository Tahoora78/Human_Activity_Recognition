DEBUG: 2023-08-02 14:51:08,130: 62444462.py: <module>: /home/tahoora/Projects/college_project/Human_Activity_Recognition/notebooks/../logs/rnn_lstm_logs/deep-conv-lstm-20230802-145108/deep-conv-lstm-20230802-145108.log
DEBUG: 2023-08-02 14:51:20,713: 62444462.py: <module>: X_train.shape=(7406, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-08-02 14:51:20,714: 62444462.py: <module>: y_train.shape=(7406, 1) y_test.shape=(2993, 1)
DEBUG: 2023-08-02 14:51:20,721: utils.py: check_class_balance: train labels
DEBUG: 2023-08-02 14:51:20,721: utils.py: check_class_balance: LAYING (0): 1412 samples (19.07 %)
DEBUG: 2023-08-02 14:51:20,722: utils.py: check_class_balance: WALKING (1): 1224 samples (16.53 %)
DEBUG: 2023-08-02 14:51:20,723: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 1072 samples (14.47 %)
DEBUG: 2023-08-02 14:51:20,724: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 987 samples (13.33 %)
DEBUG: 2023-08-02 14:51:20,726: utils.py: check_class_balance: SITTING (4): 1290 samples (17.42 %)
DEBUG: 2023-08-02 14:51:20,726: utils.py: check_class_balance: STANDING (5): 1421 samples (19.19 %)
DEBUG: 2023-08-02 14:51:20,727: utils.py: check_class_balance: test labels
DEBUG: 2023-08-02 14:51:20,728: utils.py: check_class_balance: LAYING (0): 545 samples (18.21 %)
DEBUG: 2023-08-02 14:51:20,729: utils.py: check_class_balance: WALKING (1): 496 samples (16.57 %)
DEBUG: 2023-08-02 14:51:20,730: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 470 samples (15.7 %)
DEBUG: 2023-08-02 14:51:20,732: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 419 samples (14.0 %)
DEBUG: 2023-08-02 14:51:20,733: utils.py: check_class_balance: SITTING (4): 507 samples (16.94 %)
DEBUG: 2023-08-02 14:51:20,735: utils.py: check_class_balance: STANDING (5): 556 samples (18.58 %)
DEBUG: 2023-08-02 14:51:20,766: 582463155.py: <module>: dcl_params={'batch_size': 128, 'epochs': 10000, 'lr': 0.001, 'verbose': 0}
DEBUG: 2023-08-02 14:51:20,807: 2623015374.py: <module>: X_tr.shape=(5924, 128, 6, 1) X_val.shape=(1482, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-08-02 14:51:20,808: 2623015374.py: <module>: y_tr.shape=(5924, 6) y_val.shape=(1482, 6) y_test.shape=(2993, 6)
WARNING: 2023-08-02 14:51:23,568: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-08-02 14:51:28,837: attrs.py: create: Creating converter from 5 to 3
DEBUG: 2023-08-02 14:51:32,866: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.2384 - accuracy: 0.9387 - val_loss: 0.1922 - val_accuracy: 0.9494
DEBUG: 2023-08-02 14:51:37,071: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1895 - accuracy: 0.9462 - val_loss: 0.1806 - val_accuracy: 0.9541
DEBUG: 2023-08-02 14:51:41,275: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.1545 - accuracy: 0.9504 - val_loss: 0.1441 - val_accuracy: 0.9602
DEBUG: 2023-08-02 14:51:45,457: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1516 - accuracy: 0.9497 - val_loss: 0.146 - val_accuracy: 0.9548
DEBUG: 2023-08-02 14:51:49,711: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.1893 - accuracy: 0.9407 - val_loss: 0.1418 - val_accuracy: 0.9636
DEBUG: 2023-08-02 14:51:53,960: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.1264 - accuracy: 0.9561 - val_loss: 0.1175 - val_accuracy: 0.9622
DEBUG: 2023-08-02 14:51:58,248: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.1372 - accuracy: 0.9558 - val_loss: 0.1185 - val_accuracy: 0.9615
DEBUG: 2023-08-02 14:52:02,435: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.1147 - accuracy: 0.9546 - val_loss: 0.107 - val_accuracy: 0.9622
DEBUG: 2023-08-02 14:52:06,752: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.1068 - accuracy: 0.9573 - val_loss: 0.0998 - val_accuracy: 0.9669
DEBUG: 2023-08-02 14:52:10,929: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.1386 - accuracy: 0.9517 - val_loss: 0.1511 - val_accuracy: 0.9528
DEBUG: 2023-08-02 14:52:15,276: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.1046 - accuracy: 0.9642 - val_loss: 0.1139 - val_accuracy: 0.9629
DEBUG: 2023-08-02 14:52:19,370: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.1906 - accuracy: 0.9424 - val_loss: 0.2031 - val_accuracy: 0.9447
DEBUG: 2023-08-02 14:52:23,494: keras_callback.py: on_epoch_end: Epoch 130/10000 - loss: 0.097 - accuracy: 0.9671 - val_loss: 0.0929 - val_accuracy: 0.9737
DEBUG: 2023-08-02 14:52:27,652: keras_callback.py: on_epoch_end: Epoch 140/10000 - loss: 0.0933 - accuracy: 0.9706 - val_loss: 0.0761 - val_accuracy: 0.9804
DEBUG: 2023-08-02 14:52:31,799: keras_callback.py: on_epoch_end: Epoch 150/10000 - loss: 0.0973 - accuracy: 0.9659 - val_loss: 0.0866 - val_accuracy: 0.975
DEBUG: 2023-08-02 14:52:35,910: keras_callback.py: on_epoch_end: Epoch 160/10000 - loss: 0.0923 - accuracy: 0.9696 - val_loss: 0.1156 - val_accuracy: 0.9656
DEBUG: 2023-08-02 14:52:40,134: keras_callback.py: on_epoch_end: Epoch 170/10000 - loss: 0.1977 - accuracy: 0.9377 - val_loss: 0.1314 - val_accuracy: 0.9649
DEBUG: 2023-08-02 14:52:44,337: keras_callback.py: on_epoch_end: Epoch 180/10000 - loss: 0.0807 - accuracy: 0.9748 - val_loss: 0.0746 - val_accuracy: 0.9825
DEBUG: 2023-08-02 14:52:48,501: keras_callback.py: on_epoch_end: Epoch 190/10000 - loss: 0.1078 - accuracy: 0.9689 - val_loss: 0.076 - val_accuracy: 0.9764
DEBUG: 2023-08-02 14:52:52,698: keras_callback.py: on_epoch_end: Epoch 200/10000 - loss: 0.0967 - accuracy: 0.9694 - val_loss: 0.1239 - val_accuracy: 0.9642
DEBUG: 2023-08-02 14:52:56,915: keras_callback.py: on_epoch_end: Epoch 210/10000 - loss: 0.0999 - accuracy: 0.9696 - val_loss: 0.0859 - val_accuracy: 0.9744
DEBUG: 2023-08-02 14:53:01,375: keras_callback.py: on_epoch_end: Epoch 220/10000 - loss: 0.1368 - accuracy: 0.9482 - val_loss: 0.1091 - val_accuracy: 0.9588
DEBUG: 2023-08-02 14:53:03,740: attrs.py: __getitem__: Creating converter from 3 to 5
DEBUG: 2023-08-02 14:53:08,428: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-08-02 14:53:08,429: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-08-02 14:53:08,807: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-08-02 14:53:16,428: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.3697 - accuracy: 0.8947 - val_loss: 0.3102 - val_accuracy: 0.9163
DEBUG: 2023-08-02 14:53:20,636: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.3005 - accuracy: 0.9159 - val_loss: 0.286 - val_accuracy: 0.9223
DEBUG: 2023-08-02 14:53:24,993: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.2757 - accuracy: 0.9198 - val_loss: 0.2427 - val_accuracy: 0.9379
DEBUG: 2023-08-02 14:53:29,223: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1983 - accuracy: 0.9457 - val_loss: 0.1726 - val_accuracy: 0.9514
DEBUG: 2023-08-02 14:53:33,456: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.3576 - accuracy: 0.9046 - val_loss: 0.2065 - val_accuracy: 0.9433
DEBUG: 2023-08-02 14:53:37,745: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.1404 - accuracy: 0.9571 - val_loss: 0.1418 - val_accuracy: 0.9588
DEBUG: 2023-08-02 14:53:41,962: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.1431 - accuracy: 0.9527 - val_loss: 0.1468 - val_accuracy: 0.9581
DEBUG: 2023-08-02 14:53:46,328: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.1177 - accuracy: 0.9592 - val_loss: 0.107 - val_accuracy: 0.9588
DEBUG: 2023-08-02 14:53:50,554: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.108 - accuracy: 0.9597 - val_loss: 0.0954 - val_accuracy: 0.9602
DEBUG: 2023-08-02 14:53:54,852: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.2056 - accuracy: 0.9377 - val_loss: 0.1095 - val_accuracy: 0.9642
DEBUG: 2023-08-02 14:53:59,271: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.1113 - accuracy: 0.9581 - val_loss: 0.1392 - val_accuracy: 0.9595
DEBUG: 2023-08-02 14:54:03,454: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.1218 - accuracy: 0.9549 - val_loss: 0.0973 - val_accuracy: 0.9622
DEBUG: 2023-08-02 14:54:07,951: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-08-02 14:54:07,951: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-08-02 14:54:08,272: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-08-02 14:54:15,749: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.6471 - accuracy: 0.732 - val_loss: 0.64 - val_accuracy: 0.7407
DEBUG: 2023-08-02 14:54:20,006: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.4522 - accuracy: 0.868 - val_loss: 0.3671 - val_accuracy: 0.8899
DEBUG: 2023-08-02 14:54:24,306: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.2428 - accuracy: 0.936 - val_loss: 0.2165 - val_accuracy: 0.9446
DEBUG: 2023-08-02 14:54:28,631: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.2596 - accuracy: 0.9286 - val_loss: 0.1965 - val_accuracy: 0.9399
DEBUG: 2023-08-02 14:54:32,873: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.271 - accuracy: 0.9146 - val_loss: 0.2435 - val_accuracy: 0.9298
DEBUG: 2023-08-02 14:54:37,171: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.5 - accuracy: 0.8716 - val_loss: 0.2682 - val_accuracy: 0.9217
DEBUG: 2023-08-02 14:54:45,419: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-08-02 14:54:45,420: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-08-02 14:54:45,755: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-08-02 14:54:53,462: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.6015 - accuracy: 0.8091 - val_loss: 0.6363 - val_accuracy: 0.786
DEBUG: 2023-08-02 14:54:57,810: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.4277 - accuracy: 0.8783 - val_loss: 0.341 - val_accuracy: 0.9102
DEBUG: 2023-08-02 14:55:02,164: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.2144 - accuracy: 0.9451 - val_loss: 0.2225 - val_accuracy: 0.944
DEBUG: 2023-08-02 14:55:06,459: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1604 - accuracy: 0.9544 - val_loss: 0.169 - val_accuracy: 0.9487
DEBUG: 2023-08-02 14:55:10,713: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.163 - accuracy: 0.9548 - val_loss: 0.1593 - val_accuracy: 0.9487
DEBUG: 2023-08-02 14:55:14,891: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.1628 - accuracy: 0.9522 - val_loss: 0.1415 - val_accuracy: 0.9534
DEBUG: 2023-08-02 14:55:19,228: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.1466 - accuracy: 0.9541 - val_loss: 0.1157 - val_accuracy: 0.9595
DEBUG: 2023-08-02 14:55:23,581: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.1096 - accuracy: 0.9607 - val_loss: 0.1006 - val_accuracy: 0.9615
DEBUG: 2023-08-02 14:55:27,793: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.1227 - accuracy: 0.9586 - val_loss: 0.1448 - val_accuracy: 0.9554
DEBUG: 2023-08-02 14:55:32,074: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.1432 - accuracy: 0.9512 - val_loss: 0.126 - val_accuracy: 0.9527
DEBUG: 2023-08-02 14:55:36,424: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.2395 - accuracy: 0.933 - val_loss: 0.1649 - val_accuracy: 0.948
DEBUG: 2023-08-02 14:55:41,364: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-08-02 14:55:41,364: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-08-02 14:55:41,683: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-08-02 14:55:49,255: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.3887 - accuracy: 0.9003 - val_loss: 0.3204 - val_accuracy: 0.9149
DEBUG: 2023-08-02 14:55:53,584: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.2181 - accuracy: 0.9408 - val_loss: 0.1898 - val_accuracy: 0.9507
DEBUG: 2023-08-02 14:55:57,877: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.1562 - accuracy: 0.9526 - val_loss: 0.1574 - val_accuracy: 0.9534
DEBUG: 2023-08-02 14:56:02,127: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.2363 - accuracy: 0.9269 - val_loss: 0.2042 - val_accuracy: 0.9386
DEBUG: 2023-08-02 14:56:06,373: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.1549 - accuracy: 0.948 - val_loss: 0.1647 - val_accuracy: 0.9494
DEBUG: 2023-08-02 14:56:10,697: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.1254 - accuracy: 0.9576 - val_loss: 0.117 - val_accuracy: 0.9575
DEBUG: 2023-08-02 14:56:15,053: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.1114 - accuracy: 0.9598 - val_loss: 0.1453 - val_accuracy: 0.9561
DEBUG: 2023-08-02 14:56:19,358: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.1383 - accuracy: 0.9526 - val_loss: 0.1439 - val_accuracy: 0.9494
DEBUG: 2023-08-02 14:56:23,609: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.2215 - accuracy: 0.9403 - val_loss: 0.8998 - val_accuracy: 0.788
DEBUG: 2023-08-02 14:56:28,323: 2155350369.py: <module>: ---Cross Validation Scores---
DEBUG: 2023-08-02 14:56:28,346: 2155350369.py: <module>: ---train---
DEBUG: 2023-08-02 14:56:28,348: 2155350369.py: <module>: logloss=0.101847
DEBUG: 2023-08-02 14:56:28,349: 2155350369.py: <module>: accuracy=0.964421
DEBUG: 2023-08-02 14:56:28,351: 2155350369.py: <module>: precision=0.967818
DEBUG: 2023-08-02 14:56:28,353: 2155350369.py: <module>: recall=0.967228
DEBUG: 2023-08-02 14:56:28,355: 2155350369.py: <module>: f1=0.967358
DEBUG: 2023-08-02 14:56:28,356: 2155350369.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.997767, 'WALKING_UPSTAIRS': 0.999184, 'WALKING_DOWNSTAIRS': 0.998858, 'SITTING': 0.898922, 'STANDING': 0.909415}
DEBUG: 2023-08-02 14:56:28,358: 2155350369.py: <module>: ---valid---
DEBUG: 2023-08-02 14:56:28,359: 2155350369.py: <module>: logloss=0.107586
DEBUG: 2023-08-02 14:56:28,361: 2155350369.py: <module>: accuracy=0.961785
DEBUG: 2023-08-02 14:56:28,363: 2155350369.py: <module>: precision=0.964977
DEBUG: 2023-08-02 14:56:28,365: 2155350369.py: <module>: recall=0.964703
DEBUG: 2023-08-02 14:56:28,367: 2155350369.py: <module>: f1=0.964762
DEBUG: 2023-08-02 14:56:28,369: 2155350369.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.995539, 'WALKING_UPSTAIRS': 0.998129, 'WALKING_DOWNSTAIRS': 0.996445, 'SITTING': 0.894418, 'STANDING': 0.904042}
DEBUG: 2023-08-02 14:56:28,371: 2155350369.py: <module>: ---test---
DEBUG: 2023-08-02 14:56:28,372: 2155350369.py: <module>: logloss=0.376321
DEBUG: 2023-08-02 14:56:28,373: 2155350369.py: <module>: accuracy=0.938323
DEBUG: 2023-08-02 14:56:28,374: 2155350369.py: <module>: precision=0.938964
DEBUG: 2023-08-02 14:56:28,375: 2155350369.py: <module>: recall=0.940252
DEBUG: 2023-08-02 14:56:28,377: 2155350369.py: <module>: f1=0.939039
DEBUG: 2023-08-02 14:56:28,378: 2155350369.py: <module>: per-class f1={'LAYING': 0.992916, 'WALKING': 0.941314, 'WALKING_UPSTAIRS': 0.960617, 'WALKING_DOWNSTAIRS': 0.960736, 'SITTING': 0.879333, 'STANDING': 0.899318}
DEBUG: 2023-08-02 14:56:28,388: 1672468834.py: <module>: ---Final Test Scores Averaged over Folds---
DEBUG: 2023-08-02 14:56:28,392: 1672468834.py: <module>: accuracy=0.9388573337788172
DEBUG: 2023-08-02 14:56:28,399: 1672468834.py: <module>: precision=0.939914600463765
DEBUG: 2023-08-02 14:56:28,404: 1672468834.py: <module>: recall=0.9411122468600932
DEBUG: 2023-08-02 14:56:28,408: 1672468834.py: <module>: f1=0.9401164603760747
DEBUG: 2023-08-02 14:56:28,413: 1672468834.py: <module>: per-class f1=[0.99181074 0.95933264 0.97446809 0.96313364 0.86587771 0.88607595]
