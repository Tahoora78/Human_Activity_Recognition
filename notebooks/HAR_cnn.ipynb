{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 14:16:44.978720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 14:16:46.319804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 14:16:49.045712: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "# print(tf.__version__)\n",
    "print('1: ', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m     22\u001b[0m \u001b[39m# sys.path.insert(0, '../')\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_prep\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m load_raw_data\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_class_balance, \u001b[39mround\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_feature_importance, plot_shap_summary, plot_confusion_matrix\n",
      "File \u001b[0;32m~/Projects/Human_Activity_Recognition/notebooks/../src/data_prep/load.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_prep\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocess_raw_data\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess_raw_data\n\u001b[1;32m     10\u001b[0m CUR_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m))  \u001b[39m# Path to current directory\u001b[39;00m\n\u001b[1;32m     11\u001b[0m DATA_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(CUR_DIR, \u001b[39m\"\u001b[39m\u001b[39m../../data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Human_Activity_Recognition/notebooks/../src/data_prep/preprocess_raw_data.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m MinMaxScaler, StandardScaler\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_prep\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m Preprocess\n\u001b[1;32m     13\u001b[0m CUR_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39m))  \u001b[39m# Path to current directory\u001b[39;00m\n\u001b[1;32m     14\u001b[0m DATA_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(CUR_DIR, \u001b[39m\"\u001b[39m\u001b[39m../../data/\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Path to dataset directory\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Human_Activity_Recognition/notebooks/../src/data_prep/preprocessing.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfftpack\u001b[39;00m \u001b[39mimport\u001b[39;00m fft\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal\u001b[39;00m \u001b[39mimport\u001b[39;00m butter, filtfilt\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstatsmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mempirical_distribution\u001b[39;00m \u001b[39mimport\u001b[39;00m ECDF\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstatsmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregression\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m burg\n\u001b[1;32m     16\u001b[0m logger \u001b[39m=\u001b[39m getLogger(\u001b[39m__name__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from logging import basicConfig, getLogger, StreamHandler, DEBUG, WARNING\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "\n",
    "# sys.path.insert(0, '../')\n",
    "\n",
    "from src.data_prep.load import load_raw_data\n",
    "from src.utils import check_class_balance, round\n",
    "from src.utils import plot_feature_importance, plot_shap_summary, plot_confusion_matrix\n",
    "from models.cnn import train_and_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T14:24:16.229803Z",
     "start_time": "2023-04-11T14:23:47.416742Z"
    },
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/tahoora/tensorflowEnv/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/tahoora/tensorflowEnv/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CUR_DIR = os.path.dirname(os.path.abspath(__file__))  # Path to current directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logging settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m EXEC_TIME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m LOG_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CUR_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXEC_TIME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(LOG_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create log directory\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "EXEC_TIME = \"cnn-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_DIR = os.path.join(CUR_DIR, f\"logs/{EXEC_TIME}\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)  # Create log directory\n",
    "\n",
    "formatter = \"%(levelname)s: %(asctime)s: %(filename)s: %(funcName)s: %(message)s\"\n",
    "basicConfig(filename=f\"{LOG_DIR}/{EXEC_TIME}.log\", level=DEBUG, format=formatter)\n",
    "mpl_logger = getLogger(\"matplotlib\")  # Suppress matplotlib logging\n",
    "mpl_logger.setLevel(WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Handle logging to both logging and stdout.\n",
    "getLogger().addHandler(StreamHandler(sys.stdout))\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.debug(f\"{LOG_DIR}/{EXEC_TIME}.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, label2act, act2label = load_raw_data()\n",
    "logger.debug(f\"{X_train.shape=} {X_test.shape=}\")\n",
    "logger.debug(f\"{y_train.shape=} {y_test.shape=}\")\n",
    "\n",
    "check_class_balance(y_train.flatten(), y_test.flatten(), label2act=label2act)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Split data by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=71)\n",
    "valid_preds = np.zeros((X_train.shape[0], 6))\n",
    "test_preds = np.zeros((n_splits, X_test.shape[0], 6))\n",
    "models = []\n",
    "scores: Dict[str, Dict[str, List[Any]]] = {\n",
    "    \"logloss\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"accuracy\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"precision\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"recall\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"f1\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"cm\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"per_class_f1\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(CUR_DIR, \"configs/default.json\"), \"r\") as f:\n",
    "    cnn_params = json.load(f)[\"cnn_params\"]\n",
    "    logger.debug(f\"{cnn_params=}\")\n",
    "\n",
    "y_test = keras.utils.to_categorical(y_test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n",
    "    X_tr = X_train[train_index, :]\n",
    "    X_val = X_train[valid_index, :]\n",
    "    y_tr = y_train[train_index]\n",
    "    y_val = y_train[valid_index]\n",
    "\n",
    "    y_tr = keras.utils.to_categorical(y_tr, 6)\n",
    "    y_val = keras.utils.to_categorical(y_val, 6)\n",
    "\n",
    "    logger.debug(f\"{X_tr.shape=} {X_val.shape=} {X_test.shape=}\")\n",
    "    logger.debug(f\"{y_tr.shape=} {y_val.shape=} {y_test.shape=}\")\n",
    "\n",
    "    pred_tr, pred_val, pred_test, model = train_and_predict(\n",
    "        LOG_DIR, fold_id, X_tr, X_val, X_test, y_tr, y_val, cnn_params\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    valid_preds[valid_index] = pred_val\n",
    "    test_preds[fold_id] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pred, X, y, mode in zip(\n",
    "        [pred_tr, pred_val, pred_test], [X_tr, X_val, X_test], [y_tr, y_val, y_test], [\"train\", \"valid\", \"test\"]\n",
    "    ):\n",
    "        loss, acc = model.evaluate(X, y, verbose=0)\n",
    "        pred = pred.argmax(axis=1)\n",
    "        y = y.argmax(axis=1)\n",
    "        scores[\"logloss\"][mode].append(loss)\n",
    "        scores[\"accuracy\"][mode].append(acc)\n",
    "        scores[\"precision\"][mode].append(precision_score(y, pred, average=\"macro\"))\n",
    "        scores[\"recall\"][mode].append(recall_score(y, pred, average=\"macro\"))\n",
    "        scores[\"f1\"][mode].append(f1_score(y, pred, average=\"macro\"))\n",
    "        scores[\"cm\"][mode].append(confusion_matrix(y, pred, normalize=\"true\"))\n",
    "        scores[\"per_class_f1\"][mode].append(f1_score(y, pred, average=None))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Output Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.debug(\"---Cross Validation Scores---\")\n",
    "for mode in [\"train\", \"valid\", \"test\"]:\n",
    "    logger.debug(f\"---{mode}---\")\n",
    "    for metric in [\"logloss\", \"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "        logger.debug(f\"{metric}={round(np.mean(scores[metric][mode]))}\")\n",
    "\n",
    "    class_f1_mat = scores[\"per_class_f1\"][mode]\n",
    "    class_f1_result = {}\n",
    "    for class_id in range(6):\n",
    "        mean_class_f1 = np.mean([class_f1_mat[i][class_id] for i in range(n_splits)])\n",
    "        class_f1_result[label2act[class_id]] = mean_class_f1\n",
    "    logger.debug(f\"per-class f1={round(class_f1_result)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Output Final Scores Averaged over Folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.debug(\"---Final Test Scores Averaged over Folds---\")\n",
    "test_pred = np.mean(test_preds, axis=0).argmax(axis=1)  # average over folds\n",
    "y_test = y_test.argmax(axis=1)\n",
    "logger.debug(f\"accuracy={accuracy_score(y_test, test_pred)}\")\n",
    "logger.debug(f\"precision={precision_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"recall={recall_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"f1={f1_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"per-class f1={f1_score(y_test, test_pred, average=None)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plot comfusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    cms=scores[\"cm\"],\n",
    "    labels=[\n",
    "        \"LAYING\",\n",
    "        \"WALKING\",\n",
    "        \"WALKING_UPSTAIRS\",\n",
    "        \"WALKING_DOWNSTAIRS\",\n",
    "        \"SITTING\",\n",
    "        \"STANDING\",\n",
    "    ],\n",
    "    path=f\"{LOG_DIR}/comfusion_matrix.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(f\"{LOG_DIR}/valid_oof.npy\", valid_preds)\n",
    "np.save(f\"{LOG_DIR}/test_oof.npy\", np.mean(test_preds, axis=0))  # Averaging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
