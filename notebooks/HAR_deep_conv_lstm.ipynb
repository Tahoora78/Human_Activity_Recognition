{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m../\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_prep\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m load_raw_data\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_class_balance, \u001b[39mround\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_feature_importance, plot_shap_summary, plot_confusion_matrix\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeep_conv_lstm\u001b[39;00m \u001b[39mimport\u001b[39;00m train_and_predict\n",
      "File \u001b[0;32m~/Projects/college_project/Human_Activity_Recognition/notebooks/../src/utils.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m     16\u001b[0m shap\u001b[39m.\u001b[39minitjs()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/shap/__init__.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mif\u001b[39;00m (sys\u001b[39m.\u001b[39mversion_info \u001b[39m<\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m0\u001b[39m)):\n\u001b[1;32m     10\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAs of version 0.29.0 shap only supports Python 3 (not 2)!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_explanation\u001b[39;00m \u001b[39mimport\u001b[39;00m Explanation, Cohorts\n\u001b[1;32m     14\u001b[0m \u001b[39m# explainers\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexplainers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_explainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Explainer\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/shap/_explanation.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mslicer\u001b[39;00m \u001b[39mimport\u001b[39;00m Slicer, Alias, Obj\n\u001b[1;32m     11\u001b[0m \u001b[39m# from ._order import Order\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m OpChain\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_exceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DimensionError\n\u001b[1;32m     15\u001b[0m \u001b[39m# slicer confuses pylint...\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# pylint: disable=no-member\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/shap/utils/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_clustering\u001b[39;00m \u001b[39mimport\u001b[39;00m hclust_ordering, partition_tree, partition_tree_shuffle, delta_minimization_order, hclust\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m approximate_interactions, potential_interactions, sample, safe_isinstance, assert_import, record_import_error\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_general\u001b[39;00m \u001b[39mimport\u001b[39;00m shapley_coefficients, convert_name, format_value, ordinal_str, OpChain, suppress_stderr\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/shap/utils/_clustering.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m pdist\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m jit\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from logging import basicConfig, getLogger, StreamHandler, DEBUG, WARNING\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from src.data_prep.load import load_raw_data\n",
    "from src.utils import check_class_balance, round\n",
    "from src.utils import plot_feature_importance, plot_shap_summary, plot_confusion_matrix\n",
    "from models.deep_conv_lstm import train_and_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/zsh: /home/tahoora/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /usr/bin/zsh)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/tahoora/.local/lib/python3.8/site-packages (23.2.1)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/install/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting install\n",
      "  Using cached install-1.3.5-py3-none-any.whl (3.2 kB)\n",
      "Collecting numba\n",
      "  Obtaining dependency information for numba from https://files.pythonhosted.org/packages/e6/92/ec69774aea47013fff1dfcbec891b47ed4b6f7a5619e12a9f71a5078617b/numba-0.57.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading numba-0.57.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba)\n",
      "  Obtaining dependency information for llvmlite<0.41,>=0.40.0dev0 from https://files.pythonhosted.org/packages/0e/36/9dd888e74f1fef4abffc9d421de22f729e2eb6a753e0df91a82a56ae9b08/llvmlite-0.40.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading llvmlite-0.40.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: numpy<1.25,>=1.21 in /home/tahoora/.local/lib/python3.8/site-packages (from numba) (1.24.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata->numba) (1.0.0)\n",
      "Downloading numba-0.57.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m260.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.40.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/42.1 MB\u001b[0m \u001b[31m244.9 kB/s\u001b[0m eta \u001b[36m0:01:18\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DIR = os.path.dirname(os.path.abspath('__file__'))  # Path to current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_TIME = \"deep-conv-lstm-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_DIR = os.path.join(CUR_DIR, f\"../logs/deep_cnn_lstm_logs/{EXEC_TIME}\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)  # Create log directory\n",
    "\n",
    "formatter = \"%(levelname)s: %(asctime)s: %(filename)s: %(funcName)s: %(message)s\"\n",
    "basicConfig(filename=f\"{LOG_DIR}/{EXEC_TIME}.log\", level=DEBUG, format=formatter)\n",
    "mpl_logger = getLogger(\"matplotlib\")  # Suppress matplotlib logging\n",
    "mpl_logger.setLevel(WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle logging to both logging and stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLogger().addHandler(StreamHandler(sys.stdout))\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.debug(f\"{LOG_DIR}/{EXEC_TIME}.log\")\n",
    "\n",
    "X_train, X_test, y_train, y_test, label2act, act2label = load_raw_data()\n",
    "logger.debug(f\"{X_train.shape=} {X_test.shape=}\")\n",
    "logger.debug(f\"{y_train.shape=} {y_test.shape=}\")\n",
    "\n",
    "check_class_balance(y_train.flatten(), y_test.flatten(), label2act=label2act)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=71)\n",
    "valid_preds = np.zeros((X_train.shape[0], 6))\n",
    "test_preds = np.zeros((n_splits, X_test.shape[0], 6))\n",
    "models = []\n",
    "scores: Dict[str, Dict[str, List[Any]]] = {\n",
    "    \"logloss\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"accuracy\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"precision\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"recall\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"f1\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"cm\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"per_class_f1\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(CUR_DIR, \"../configs/default.json\"), \"r\") as f:\n",
    "    dcl_params = json.load(f)[\"deep_conv_lstm_params\"]\n",
    "    logger.debug(f\"{dcl_params=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = keras.utils.to_categorical(y_test, 6)\n",
    "\n",
    "for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n",
    "    X_tr = X_train[train_index, :]\n",
    "    X_val = X_train[valid_index, :]\n",
    "    y_tr = y_train[train_index]\n",
    "    y_val = y_train[valid_index]\n",
    "    \n",
    "    y_tr = keras.utils.to_categorical(y_tr, 6)\n",
    "    y_val = keras.utils.to_categorical(y_val, 6)\n",
    "\n",
    "    logger.debug(f\"{X_tr.shape=} {X_val.shape=} {X_test.shape=}\")\n",
    "    logger.debug(f\"{y_tr.shape=} {y_val.shape=} {y_test.shape=}\")\n",
    "\n",
    "    pred_tr, pred_val, pred_test, model = train_and_predict(\n",
    "        LOG_DIR, fold_id, X_tr, X_val, X_test, y_tr, y_val, dcl_params\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    valid_preds[valid_index] = pred_val\n",
    "    test_preds[fold_id] = pred_test\n",
    "\n",
    "    for pred, X, y, mode in zip(\n",
    "        [pred_tr, pred_val, pred_test], [X_tr, X_val, X_test], [y_tr, y_val, y_test], [\"train\", \"valid\", \"test\"]\n",
    "    ):\n",
    "        loss, acc = model.evaluate(X, y, verbose=0)\n",
    "        pred = pred.argmax(axis=1)\n",
    "        y = y.argmax(axis=1)\n",
    "        scores[\"logloss\"][mode].append(loss)\n",
    "        scores[\"accuracy\"][mode].append(acc)\n",
    "        scores[\"precision\"][mode].append(precision_score(y, pred, average=\"macro\"))\n",
    "        scores[\"recall\"][mode].append(recall_score(y, pred, average=\"macro\"))\n",
    "        scores[\"f1\"][mode].append(f1_score(y, pred, average=\"macro\"))\n",
    "        scores[\"cm\"][mode].append(confusion_matrix(y, pred, normalize=\"true\"))\n",
    "        scores[\"per_class_f1\"][mode].append(f1_score(y, pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output cross validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"---Cross Validation Scores---\")\n",
    "for mode in [\"train\", \"valid\", \"test\"]:\n",
    "    logger.debug(f\"---{mode}---\")\n",
    "    for metric in [\"logloss\", \"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "        logger.debug(f\"{metric}={round(np.mean(scores[metric][mode]))}\")\n",
    "\n",
    "    class_f1_mat = scores[\"per_class_f1\"][mode]\n",
    "    class_f1_result = {}\n",
    "    for class_id in range(6):\n",
    "        mean_class_f1 = np.mean([class_f1_mat[i][class_id] for i in range(n_splits)])\n",
    "        class_f1_result[label2act[class_id]] = mean_class_f1\n",
    "    logger.debug(f\"per-class f1={round(class_f1_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output final scores averaged over folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"---Final Test Scores Averaged over Folds---\")\n",
    "test_pred = np.mean(test_preds, axis=0).argmax(axis=1)  # average over folds\n",
    "y_test = y_test.argmax(axis=1)\n",
    "logger.debug(f\"accuracy={accuracy_score(y_test, test_pred)}\")\n",
    "logger.debug(f\"precision={precision_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"recall={recall_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"f1={f1_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"per-class f1={f1_score(y_test, test_pred, average=None)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    cms=scores[\"cm\"],\n",
    "    labels=[\n",
    "        \"LAYING\",\n",
    "        \"WALKING\",\n",
    "        \"WALKING_UPSTAIRS\",\n",
    "        \"WALKING_DOWNSTAIRS\",\n",
    "        \"SITTING\",\n",
    "        \"STANDING\",\n",
    "    ],\n",
    "    path=f\"{LOG_DIR}/comfusion_matrix.png\",\n",
    ")\n",
    "\n",
    "np.save(f\"{LOG_DIR}/valid_oof.npy\", valid_preds)\n",
    "np.save(f\"{LOG_DIR}/test_oof.npy\", np.mean(test_preds, axis=0))  # Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    y_predicted_train = model.predict(X_train).argmax(axis=1)\n",
    "\n",
    "    y_predicted_test = model.predict(X_test).argmax(axis=1)\n",
    "    print(\"Train acc, error\",accuracy_score( y_train, y_predicted_train), np.sqrt(metrics.mean_squared_error(y_train, y_predicted_train)), \n",
    "          \"Test acc, error\", accuracy_score( y_test, y_predicted_test), np.sqrt(metrics.mean_squared_error(y_test, y_predicted_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
