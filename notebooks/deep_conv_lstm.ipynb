{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-30 12:38:51.172699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-30 12:38:56.683228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m StratifiedKFold\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_prep\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m load_raw_data\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_class_balance, \u001b[39mround\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_feature_importance, plot_shap_summary, plot_confusion_matrix\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from logging import basicConfig, getLogger, StreamHandler, DEBUG, WARNING\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow import keras\n",
    "\n",
    "from src.data_prep.load import load_raw_data\n",
    "from src.utils import check_class_balance, round\n",
    "from src.utils import plot_feature_importance, plot_shap_summary, plot_confusion_matrix\n",
    "from models.deep_conv_lstm import train_and_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUR_DIR = os.path.dirname(os.path.abspath(__file__))  # Path to current directory\n",
    "\n",
    "# Logging settings\n",
    "EXEC_TIME = \"deep-conv-lstm-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_DIR = os.path.join(CUR_DIR, f\"logs/{EXEC_TIME}\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)  # Create log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = \"%(levelname)s: %(asctime)s: %(filename)s: %(funcName)s: %(message)s\"\n",
    "basicConfig(filename=f\"{LOG_DIR}/{EXEC_TIME}.log\", level=DEBUG, format=formatter)\n",
    "mpl_logger = getLogger(\"matplotlib\")  # Suppress matplotlib logging\n",
    "mpl_logger.setLevel(WARNING)\n",
    "# Handle logging to both logging and stdout.\n",
    "getLogger().addHandler(StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = getLogger(__name__)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.debug(f\"{LOG_DIR}/{EXEC_TIME}.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, label2act, act2label = load_raw_data()\n",
    "logger.debug(f\"{X_train.shape=} {X_test.shape=}\")\n",
    "logger.debug(f\"{y_train.shape=} {y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_class_balance(y_train.flatten(), y_test.flatten(), label2act=label2act)\n",
    "\n",
    "# Split data by preserving the percentage of samples for each class.\n",
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=71)\n",
    "valid_preds = np.zeros((X_train.shape[0], 6))\n",
    "test_preds = np.zeros((n_splits, X_test.shape[0], 6))\n",
    "models = []\n",
    "scores: Dict[str, Dict[str, List[Any]]] = {\n",
    "    \"logloss\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"accuracy\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"precision\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"recall\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"f1\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"cm\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "    \"per_class_f1\": {\"train\": [], \"valid\": [], \"test\": []},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hyper-parameters\n",
    "with open(os.path.join(CUR_DIR, \"configs/default.json\"), \"r\") as f:\n",
    "    dcl_params = json.load(f)[\"deep_conv_lstm_params\"]\n",
    "    logger.debug(f\"{dcl_params=}\")\n",
    "\n",
    "y_test = keras.utils.to_categorical(y_test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n",
    "    X_tr = X_train[train_index, :]\n",
    "    X_val = X_train[valid_index, :]\n",
    "    y_tr = y_train[train_index]\n",
    "    y_val = y_train[valid_index]\n",
    "    \n",
    "    y_tr = keras.utils.to_categorical(y_tr, 6)\n",
    "    y_val = keras.utils.to_categorical(y_val, 6)\n",
    "\n",
    "    logger.debug(f\"{X_tr.shape=} {X_val.shape=} {X_test.shape=}\")\n",
    "    logger.debug(f\"{y_tr.shape=} {y_val.shape=} {y_test.shape=}\")\n",
    "\n",
    "    pred_tr, pred_val, pred_test, model = train_and_predict(\n",
    "        LOG_DIR, fold_id, X_tr, X_val, X_test, y_tr, y_val, dcl_params\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    valid_preds[valid_index] = pred_val\n",
    "    test_preds[fold_id] = pred_test\n",
    "\n",
    "    for pred, X, y, mode in zip(\n",
    "        [pred_tr, pred_val, pred_test], [X_tr, X_val, X_test], [y_tr, y_val, y_test], [\"train\", \"valid\", \"test\"]\n",
    "    ):\n",
    "        loss, acc = model.evaluate(X, y, verbose=0)\n",
    "        pred = pred.argmax(axis=1)\n",
    "        y = y.argmax(axis=1)\n",
    "        scores[\"logloss\"][mode].append(loss)\n",
    "        scores[\"accuracy\"][mode].append(acc)\n",
    "        scores[\"precision\"][mode].append(precision_score(y, pred, average=\"macro\"))\n",
    "        scores[\"recall\"][mode].append(recall_score(y, pred, average=\"macro\"))\n",
    "        scores[\"f1\"][mode].append(f1_score(y, pred, average=\"macro\"))\n",
    "        scores[\"cm\"][mode].append(confusion_matrix(y, pred, normalize=\"true\"))\n",
    "        scores[\"per_class_f1\"][mode].append(f1_score(y, pred, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Cross Validation Scores\n",
    "logger.debug(\"---Cross Validation Scores---\")\n",
    "for mode in [\"train\", \"valid\", \"test\"]:\n",
    "    logger.debug(f\"---{mode}---\")\n",
    "    for metric in [\"logloss\", \"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "        logger.debug(f\"{metric}={round(np.mean(scores[metric][mode]))}\")\n",
    "\n",
    "    class_f1_mat = scores[\"per_class_f1\"][mode]\n",
    "    class_f1_result = {}\n",
    "    for class_id in range(6):\n",
    "        mean_class_f1 = np.mean([class_f1_mat[i][class_id] for i in range(n_splits)])\n",
    "        class_f1_result[label2act[class_id]] = mean_class_f1\n",
    "    logger.debug(f\"per-class f1={round(class_f1_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Final Scores Averaged over Folds\n",
    "logger.debug(\"---Final Test Scores Averaged over Folds---\")\n",
    "test_pred = np.mean(test_preds, axis=0).argmax(axis=1)  # average over folds\n",
    "y_test = y_test.argmax(axis=1)\n",
    "logger.debug(f\"accuracy={accuracy_score(y_test, test_pred)}\")\n",
    "logger.debug(f\"precision={precision_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"recall={recall_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"f1={f1_score(y_test, test_pred, average='macro')}\")\n",
    "logger.debug(f\"per-class f1={f1_score(y_test, test_pred, average=None)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file=f\"{LOG_DIR}/model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    cms=scores[\"cm\"],\n",
    "    labels=[\n",
    "        \"LAYING\",\n",
    "        \"WALKING\",\n",
    "        \"WALKING_UPSTAIRS\",\n",
    "        \"WALKING_DOWNSTAIRS\",\n",
    "        \"SITTING\",\n",
    "        \"STANDING\",\n",
    "    ],\n",
    "    path=f\"{LOG_DIR}/comfusion_matrix.png\",\n",
    ")\n",
    "\n",
    "np.save(f\"{LOG_DIR}/valid_oof.npy\", valid_preds)\n",
    "np.save(f\"{LOG_DIR}/test_oof.npy\", np.mean(test_preds, axis=0))  # Averaging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
