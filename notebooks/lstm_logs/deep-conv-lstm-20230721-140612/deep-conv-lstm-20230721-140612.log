DEBUG: 2023-07-21 14:06:12,457: 62444462.py: <module>: /home/tahoora/Projects/college_project/Human_Activity_Recognition/notebooks/lstm_logs/deep-conv-lstm-20230721-140612/deep-conv-lstm-20230721-140612.log
DEBUG: 2023-07-21 14:06:25,292: 62444462.py: <module>: X_train.shape=(7406, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-21 14:06:25,293: 62444462.py: <module>: y_train.shape=(7406, 1) y_test.shape=(2993, 1)
DEBUG: 2023-07-21 14:06:25,296: utils.py: check_class_balance: train labels
DEBUG: 2023-07-21 14:06:25,298: utils.py: check_class_balance: LAYING (0): 1412 samples (19.07 %)
DEBUG: 2023-07-21 14:06:25,299: utils.py: check_class_balance: WALKING (1): 1224 samples (16.53 %)
DEBUG: 2023-07-21 14:06:25,301: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 1072 samples (14.47 %)
DEBUG: 2023-07-21 14:06:25,302: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 987 samples (13.33 %)
DEBUG: 2023-07-21 14:06:25,303: utils.py: check_class_balance: SITTING (4): 1290 samples (17.42 %)
DEBUG: 2023-07-21 14:06:25,305: utils.py: check_class_balance: STANDING (5): 1421 samples (19.19 %)
DEBUG: 2023-07-21 14:06:25,307: utils.py: check_class_balance: test labels
DEBUG: 2023-07-21 14:06:25,308: utils.py: check_class_balance: LAYING (0): 545 samples (18.21 %)
DEBUG: 2023-07-21 14:06:25,308: utils.py: check_class_balance: WALKING (1): 496 samples (16.57 %)
DEBUG: 2023-07-21 14:06:25,309: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 470 samples (15.7 %)
DEBUG: 2023-07-21 14:06:25,310: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 419 samples (14.0 %)
DEBUG: 2023-07-21 14:06:25,311: utils.py: check_class_balance: SITTING (4): 507 samples (16.94 %)
DEBUG: 2023-07-21 14:06:25,312: utils.py: check_class_balance: STANDING (5): 556 samples (18.58 %)
DEBUG: 2023-07-21 14:06:25,351: 3941774885.py: <module>: dcl_params={'batch_size': 128, 'epochs': 10000, 'lr': 0.001, 'verbose': 0}
DEBUG: 2023-07-21 14:06:25,390: 3631112952.py: <module>: X_tr.shape=(5924, 128, 6, 1) X_val.shape=(1482, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-21 14:06:25,390: 3631112952.py: <module>: y_tr.shape=(5924, 6) y_val.shape=(1482, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-21 14:06:28,111: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-21 14:06:44,760: attrs.py: create: Creating converter from 5 to 3
DEBUG: 2023-07-21 14:07:08,494: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1021 - accuracy: 0.9608 - val_loss: 0.0928 - val_accuracy: 0.9649
DEBUG: 2023-07-21 14:07:33,398: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.0379 - accuracy: 0.9843 - val_loss: 0.0422 - val_accuracy: 0.9804
DEBUG: 2023-07-21 14:08:01,338: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0139 - accuracy: 0.9953 - val_loss: 0.0238 - val_accuracy: 0.9919
DEBUG: 2023-07-21 14:08:27,715: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1014 - accuracy: 0.9673 - val_loss: 0.0459 - val_accuracy: 0.9831
DEBUG: 2023-07-21 14:08:53,945: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.017 - accuracy: 0.9959 - val_loss: 0.0392 - val_accuracy: 0.9892
DEBUG: 2023-07-21 14:09:20,316: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0194 - val_accuracy: 0.9953
DEBUG: 2023-07-21 14:09:28,485: attrs.py: __getitem__: Creating converter from 3 to 5
DEBUG: 2023-07-21 14:09:33,742: 3631112952.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-21 14:09:33,742: 3631112952.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-21 14:09:34,409: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-21 14:10:08,924: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.0804 - accuracy: 0.9684 - val_loss: 0.0834 - val_accuracy: 0.9622
DEBUG: 2023-07-21 14:10:35,662: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.0777 - accuracy: 0.9683 - val_loss: 0.097 - val_accuracy: 0.9595
DEBUG: 2023-07-21 14:11:02,553: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0507 - accuracy: 0.9808 - val_loss: 0.0557 - val_accuracy: 0.9716
DEBUG: 2023-07-21 14:11:29,300: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0633 - accuracy: 0.9725 - val_loss: 0.0576 - val_accuracy: 0.9696
DEBUG: 2023-07-21 14:11:55,974: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0516 - accuracy: 0.9818 - val_loss: 0.1824 - val_accuracy: 0.9271
DEBUG: 2023-07-21 14:12:22,536: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0422 - accuracy: 0.9835 - val_loss: 0.0704 - val_accuracy: 0.975
DEBUG: 2023-07-21 14:12:49,190: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0278 - accuracy: 0.99 - val_loss: 0.0339 - val_accuracy: 0.9838
DEBUG: 2023-07-21 14:13:15,961: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.0538 - val_accuracy: 0.9811
DEBUG: 2023-07-21 14:13:42,805: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0134 - val_accuracy: 0.9953
DEBUG: 2023-07-21 14:14:09,494: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0209 - val_accuracy: 0.9946
DEBUG: 2023-07-21 14:14:36,083: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.0155 - val_accuracy: 0.9946
DEBUG: 2023-07-21 14:15:02,779: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0282 - val_accuracy: 0.9878
DEBUG: 2023-07-21 14:15:29,521: keras_callback.py: on_epoch_end: Epoch 130/10000 - loss: 0.0067 - accuracy: 0.999 - val_loss: 0.0313 - val_accuracy: 0.9905
DEBUG: 2023-07-21 14:15:56,179: keras_callback.py: on_epoch_end: Epoch 140/10000 - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.0269 - val_accuracy: 0.9912
DEBUG: 2023-07-21 14:16:22,940: keras_callback.py: on_epoch_end: Epoch 150/10000 - loss: 0.002 - accuracy: 0.9997 - val_loss: 0.0113 - val_accuracy: 0.9973
DEBUG: 2023-07-21 14:16:49,630: keras_callback.py: on_epoch_end: Epoch 160/10000 - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.0142 - val_accuracy: 0.9959
DEBUG: 2023-07-21 14:17:16,372: keras_callback.py: on_epoch_end: Epoch 170/10000 - loss: 0.0035 - accuracy: 0.999 - val_loss: 0.0124 - val_accuracy: 0.9973
DEBUG: 2023-07-21 14:17:43,040: keras_callback.py: on_epoch_end: Epoch 180/10000 - loss: 0.0002 - accuracy: 1.0 - val_loss: 0.0124 - val_accuracy: 0.9973
DEBUG: 2023-07-21 14:18:09,706: keras_callback.py: on_epoch_end: Epoch 190/10000 - loss: 0.0001 - accuracy: 1.0 - val_loss: 0.009 - val_accuracy: 0.998
DEBUG: 2023-07-21 14:18:30,823: 3631112952.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-21 14:18:30,824: 3631112952.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-21 14:18:31,558: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-21 14:19:04,853: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.0941 - accuracy: 0.9598 - val_loss: 0.1254 - val_accuracy: 0.9588
DEBUG: 2023-07-21 14:19:31,740: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.0854 - accuracy: 0.9629 - val_loss: 0.1752 - val_accuracy: 0.9338
DEBUG: 2023-07-21 14:19:58,577: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0806 - accuracy: 0.9732 - val_loss: 0.0666 - val_accuracy: 0.975
DEBUG: 2023-07-21 14:20:25,653: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0096 - accuracy: 0.997 - val_loss: 0.0215 - val_accuracy: 0.9899
DEBUG: 2023-07-21 14:20:52,297: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.0291 - val_accuracy: 0.9926
DEBUG: 2023-07-21 14:21:19,049: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.008 - accuracy: 0.9971 - val_loss: 0.0396 - val_accuracy: 0.9878
DEBUG: 2023-07-21 14:21:45,826: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0048 - accuracy: 0.999 - val_loss: 0.0287 - val_accuracy: 0.9905
DEBUG: 2023-07-21 14:22:12,596: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0207 - val_accuracy: 0.9939
DEBUG: 2023-07-21 14:22:31,292: 3631112952.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-21 14:22:31,293: 3631112952.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-21 14:22:32,338: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-21 14:23:05,826: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.0823 - accuracy: 0.9686 - val_loss: 0.07 - val_accuracy: 0.9723
DEBUG: 2023-07-21 14:23:32,796: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.0656 - accuracy: 0.9757 - val_loss: 0.0829 - val_accuracy: 0.9716
DEBUG: 2023-07-21 14:23:59,876: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0308 - accuracy: 0.9887 - val_loss: 0.0298 - val_accuracy: 0.9885
DEBUG: 2023-07-21 14:24:26,676: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.028 - accuracy: 0.9895 - val_loss: 0.0263 - val_accuracy: 0.9919
DEBUG: 2023-07-21 14:24:53,551: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0267 - accuracy: 0.9907 - val_loss: 0.0255 - val_accuracy: 0.9912
DEBUG: 2023-07-21 14:25:20,283: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0262 - accuracy: 0.9902 - val_loss: 0.0228 - val_accuracy: 0.9905
DEBUG: 2023-07-21 14:25:47,032: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.006 - accuracy: 0.9985 - val_loss: 0.0192 - val_accuracy: 0.9946
DEBUG: 2023-07-21 14:26:13,741: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0341 - val_accuracy: 0.9926
DEBUG: 2023-07-21 14:26:40,482: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0017 - accuracy: 0.9992 - val_loss: 0.0197 - val_accuracy: 0.9959
DEBUG: 2023-07-21 14:27:07,140: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0348 - val_accuracy: 0.9939
DEBUG: 2023-07-21 14:27:33,776: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0155 - val_accuracy: 0.9966
DEBUG: 2023-07-21 14:27:46,847: 3631112952.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-21 14:27:46,847: 3631112952.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-21 14:27:47,593: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-21 14:28:21,637: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.0885 - accuracy: 0.9637 - val_loss: 0.0739 - val_accuracy: 0.9676
DEBUG: 2023-07-21 14:28:48,653: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1041 - accuracy: 0.9632 - val_loss: 0.1192 - val_accuracy: 0.9554
DEBUG: 2023-07-21 14:29:15,590: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0507 - accuracy: 0.9796 - val_loss: 0.0436 - val_accuracy: 0.9818
DEBUG: 2023-07-21 14:29:42,384: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0728 - accuracy: 0.973 - val_loss: 0.057 - val_accuracy: 0.9784
DEBUG: 2023-07-21 14:30:09,564: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0233 - accuracy: 0.9917 - val_loss: 0.1091 - val_accuracy: 0.9784
DEBUG: 2023-07-21 14:30:36,336: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.0274 - val_accuracy: 0.9926
DEBUG: 2023-07-21 14:31:03,066: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.016 - val_accuracy: 0.9959
DEBUG: 2023-07-21 14:31:29,966: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.0195 - val_accuracy: 0.9959
DEBUG: 2023-07-21 14:31:56,738: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0468 - accuracy: 0.9846 - val_loss: 0.0308 - val_accuracy: 0.9899
DEBUG: 2023-07-21 14:32:23,416: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0124 - val_accuracy: 0.9959
DEBUG: 2023-07-21 14:32:50,139: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0005 - accuracy: 1.0 - val_loss: 0.0168 - val_accuracy: 0.9966
DEBUG: 2023-07-21 14:34:29,852: 2155350369.py: <module>: ---Cross Validation Scores---
DEBUG: 2023-07-21 14:34:29,854: 2155350369.py: <module>: ---train---
DEBUG: 2023-07-21 14:34:29,855: 2155350369.py: <module>: logloss=0.000978
DEBUG: 2023-07-21 14:34:29,856: 2155350369.py: <module>: accuracy=0.999831
DEBUG: 2023-07-21 14:34:29,857: 2155350369.py: <module>: precision=0.99983
DEBUG: 2023-07-21 14:34:29,858: 2155350369.py: <module>: recall=0.999789
DEBUG: 2023-07-21 14:34:29,859: 2155350369.py: <module>: f1=0.999809
DEBUG: 2023-07-21 14:34:37,494: 1695527446.py: <module>: ---Final Test Scores Averaged over Folds---
DEBUG: 2023-07-21 14:34:37,499: 1695527446.py: <module>: accuracy=0.947878382893418
DEBUG: 2023-07-21 14:34:37,503: 1695527446.py: <module>: precision=0.9488913592154228
DEBUG: 2023-07-21 14:34:37,507: 1695527446.py: <module>: recall=0.9498557269717328
DEBUG: 2023-07-21 14:34:37,512: 1695527446.py: <module>: f1=0.9488226010492739
DEBUG: 2023-07-21 14:34:37,517: 1695527446.py: <module>: per-class f1=[1.         0.96558916 0.98927039 0.95108077 0.88888889 0.8981064 ]
