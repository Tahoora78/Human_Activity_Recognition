DEBUG: 2023-07-23 11:41:01,975: 62444462.py: <module>: /home/tahoora/Projects/college_project/Human_Activity_Recognition/notebooks/stacked_lstm_logs/deep-conv-lstm-20230723-114101/deep-conv-lstm-20230723-114101.log
DEBUG: 2023-07-23 11:41:13,235: 62444462.py: <module>: X_train.shape=(7406, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-23 11:41:13,236: 62444462.py: <module>: y_train.shape=(7406, 1) y_test.shape=(2993, 1)
DEBUG: 2023-07-23 11:41:13,240: utils.py: check_class_balance: train labels
DEBUG: 2023-07-23 11:41:13,240: utils.py: check_class_balance: LAYING (0): 1412 samples (19.07 %)
DEBUG: 2023-07-23 11:41:13,241: utils.py: check_class_balance: WALKING (1): 1224 samples (16.53 %)
DEBUG: 2023-07-23 11:41:13,242: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 1072 samples (14.47 %)
DEBUG: 2023-07-23 11:41:13,243: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 987 samples (13.33 %)
DEBUG: 2023-07-23 11:41:13,244: utils.py: check_class_balance: SITTING (4): 1290 samples (17.42 %)
DEBUG: 2023-07-23 11:41:13,244: utils.py: check_class_balance: STANDING (5): 1421 samples (19.19 %)
DEBUG: 2023-07-23 11:41:13,245: utils.py: check_class_balance: test labels
DEBUG: 2023-07-23 11:41:13,245: utils.py: check_class_balance: LAYING (0): 545 samples (18.21 %)
DEBUG: 2023-07-23 11:41:13,247: utils.py: check_class_balance: WALKING (1): 496 samples (16.57 %)
DEBUG: 2023-07-23 11:41:13,248: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 470 samples (15.7 %)
DEBUG: 2023-07-23 11:41:13,249: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 419 samples (14.0 %)
DEBUG: 2023-07-23 11:41:13,250: utils.py: check_class_balance: SITTING (4): 507 samples (16.94 %)
DEBUG: 2023-07-23 11:41:13,251: utils.py: check_class_balance: STANDING (5): 556 samples (18.58 %)
DEBUG: 2023-07-23 11:41:34,403: 1134177668.py: <module>: dcl_params={'batch_size': 128, 'epochs': 10000, 'lr': 0.001, 'verbose': 0}
DEBUG: 2023-07-23 11:41:58,851: 222416104.py: <module>: X_tr.shape=(5924, 128, 6, 1) X_val.shape=(1482, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-23 11:41:58,852: 222416104.py: <module>: y_tr.shape=(5924, 6) y_val.shape=(1482, 6) y_test.shape=(2993, 1)
WARNING: 2023-07-23 11:42:00,940: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-23 11:42:11,902: attrs.py: create: Creating converter from 5 to 3
DEBUG: 2023-07-23 11:42:30,838: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1242 - accuracy: 0.9502 - val_loss: 0.1202 - val_accuracy: 0.9615
DEBUG: 2023-07-23 11:42:51,752: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.0975 - accuracy: 0.9556 - val_loss: 0.0973 - val_accuracy: 0.9649
DEBUG: 2023-07-23 11:43:12,711: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0769 - accuracy: 0.96 - val_loss: 0.0718 - val_accuracy: 0.969
DEBUG: 2023-07-23 11:43:33,667: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0754 - accuracy: 0.9608 - val_loss: 0.0733 - val_accuracy: 0.9703
DEBUG: 2023-07-23 11:43:54,822: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0853 - accuracy: 0.963 - val_loss: 0.103 - val_accuracy: 0.9548
DEBUG: 2023-07-23 11:44:15,846: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.1995 - accuracy: 0.9315 - val_loss: 0.1337 - val_accuracy: 0.9514
DEBUG: 2023-07-23 11:44:36,978: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0727 - accuracy: 0.9619 - val_loss: 0.0722 - val_accuracy: 0.9696
DEBUG: 2023-07-23 11:44:39,309: attrs.py: __getitem__: Creating converter from 3 to 5
DEBUG: 2023-07-23 11:44:52,099: 222416104.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-23 11:44:52,100: 222416104.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 1)
WARNING: 2023-07-23 11:44:53,132: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-23 11:45:23,462: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1177 - accuracy: 0.9532 - val_loss: 0.0954 - val_accuracy: 0.9561
DEBUG: 2023-07-23 11:45:45,122: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1573 - accuracy: 0.9438 - val_loss: 0.1277 - val_accuracy: 0.9507
DEBUG: 2023-07-23 11:46:06,761: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.1124 - accuracy: 0.957 - val_loss: 0.1171 - val_accuracy: 0.9541
DEBUG: 2023-07-23 11:46:28,048: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.084 - accuracy: 0.9634 - val_loss: 0.109 - val_accuracy: 0.9568
DEBUG: 2023-07-23 11:46:49,754: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0954 - accuracy: 0.9612 - val_loss: 0.0921 - val_accuracy: 0.9554
DEBUG: 2023-07-23 11:47:11,318: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0841 - accuracy: 0.9614 - val_loss: 0.1132 - val_accuracy: 0.9332
DEBUG: 2023-07-23 11:47:32,416: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0692 - accuracy: 0.9701 - val_loss: 0.097 - val_accuracy: 0.9595
DEBUG: 2023-07-23 11:47:53,642: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0964 - accuracy: 0.959 - val_loss: 0.0936 - val_accuracy: 0.9622
DEBUG: 2023-07-23 11:48:14,866: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0849 - accuracy: 0.9678 - val_loss: 0.0883 - val_accuracy: 0.9608
DEBUG: 2023-07-23 11:48:36,947: 222416104.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-23 11:48:36,948: 222416104.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 1)
WARNING: 2023-07-23 11:48:37,807: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-23 11:49:06,408: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1084 - accuracy: 0.9565 - val_loss: 0.1231 - val_accuracy: 0.9487
DEBUG: 2023-07-23 11:49:27,815: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1083 - accuracy: 0.9578 - val_loss: 0.1216 - val_accuracy: 0.946
DEBUG: 2023-07-23 11:49:48,829: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.113 - accuracy: 0.9566 - val_loss: 0.1524 - val_accuracy: 0.9406
DEBUG: 2023-07-23 11:50:10,077: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0977 - accuracy: 0.9603 - val_loss: 0.1148 - val_accuracy: 0.9514
DEBUG: 2023-07-23 11:50:31,182: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.087 - accuracy: 0.9615 - val_loss: 0.1025 - val_accuracy: 0.9541
DEBUG: 2023-07-23 11:50:52,455: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.1404 - accuracy: 0.9546 - val_loss: 0.132 - val_accuracy: 0.9507
DEBUG: 2023-07-23 11:51:13,638: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0812 - accuracy: 0.9592 - val_loss: 0.0959 - val_accuracy: 0.9595
DEBUG: 2023-07-23 11:51:35,402: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.088 - accuracy: 0.9578 - val_loss: 0.1052 - val_accuracy: 0.9494
DEBUG: 2023-07-23 11:51:56,983: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0705 - accuracy: 0.9637 - val_loss: 0.0831 - val_accuracy: 0.9622
DEBUG: 2023-07-23 11:52:18,481: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0578 - accuracy: 0.9696 - val_loss: 0.0742 - val_accuracy: 0.9635
DEBUG: 2023-07-23 11:52:40,051: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0519 - accuracy: 0.9764 - val_loss: 0.0732 - val_accuracy: 0.9568
DEBUG: 2023-07-23 11:53:01,627: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.0518 - accuracy: 0.9727 - val_loss: 0.058 - val_accuracy: 0.975
DEBUG: 2023-07-23 11:53:23,063: keras_callback.py: on_epoch_end: Epoch 130/10000 - loss: 0.0502 - accuracy: 0.9745 - val_loss: 0.0593 - val_accuracy: 0.975
DEBUG: 2023-07-23 11:53:44,637: keras_callback.py: on_epoch_end: Epoch 140/10000 - loss: 0.044 - accuracy: 0.976 - val_loss: 0.0512 - val_accuracy: 0.973
DEBUG: 2023-07-23 11:54:06,036: keras_callback.py: on_epoch_end: Epoch 150/10000 - loss: 0.0288 - accuracy: 0.9853 - val_loss: 0.0499 - val_accuracy: 0.9818
DEBUG: 2023-07-23 11:54:27,634: keras_callback.py: on_epoch_end: Epoch 160/10000 - loss: 0.0415 - accuracy: 0.9816 - val_loss: 0.0424 - val_accuracy: 0.9824
DEBUG: 2023-07-23 11:54:49,051: keras_callback.py: on_epoch_end: Epoch 170/10000 - loss: 0.0437 - accuracy: 0.9784 - val_loss: 0.0608 - val_accuracy: 0.9737
DEBUG: 2023-07-23 11:55:10,484: keras_callback.py: on_epoch_end: Epoch 180/10000 - loss: 0.0294 - accuracy: 0.9845 - val_loss: 0.0426 - val_accuracy: 0.9791
DEBUG: 2023-07-23 11:55:32,171: keras_callback.py: on_epoch_end: Epoch 190/10000 - loss: 0.0197 - accuracy: 0.9932 - val_loss: 0.022 - val_accuracy: 0.9878
DEBUG: 2023-07-23 11:55:53,670: keras_callback.py: on_epoch_end: Epoch 200/10000 - loss: 0.0236 - accuracy: 0.9905 - val_loss: 0.0453 - val_accuracy: 0.9845
DEBUG: 2023-07-23 11:56:15,055: keras_callback.py: on_epoch_end: Epoch 210/10000 - loss: 0.0569 - accuracy: 0.9723 - val_loss: 0.0752 - val_accuracy: 0.9608
DEBUG: 2023-07-23 11:56:36,591: keras_callback.py: on_epoch_end: Epoch 220/10000 - loss: 0.0259 - accuracy: 0.9916 - val_loss: 0.0364 - val_accuracy: 0.9872
DEBUG: 2023-07-23 11:56:51,248: 222416104.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-23 11:56:51,249: 222416104.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 1)
WARNING: 2023-07-23 11:56:52,393: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-23 11:57:22,026: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1318 - accuracy: 0.9497 - val_loss: 0.211 - val_accuracy: 0.9169
DEBUG: 2023-07-23 11:57:43,338: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.0982 - accuracy: 0.9605 - val_loss: 0.0972 - val_accuracy: 0.9581
DEBUG: 2023-07-23 11:58:04,459: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0989 - accuracy: 0.9576 - val_loss: 0.0923 - val_accuracy: 0.9575
DEBUG: 2023-07-23 11:58:25,583: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1085 - accuracy: 0.9595 - val_loss: 0.0894 - val_accuracy: 0.9554
DEBUG: 2023-07-23 11:58:46,784: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0766 - accuracy: 0.9703 - val_loss: 0.1151 - val_accuracy: 0.9568
DEBUG: 2023-07-23 11:59:07,894: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0876 - accuracy: 0.9678 - val_loss: 0.1 - val_accuracy: 0.9622
DEBUG: 2023-07-23 11:59:29,003: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0903 - accuracy: 0.9634 - val_loss: 0.0913 - val_accuracy: 0.9615
DEBUG: 2023-07-23 11:59:58,558: 222416104.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-23 11:59:58,559: 222416104.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 1)
WARNING: 2023-07-23 11:59:59,343: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-23 12:00:27,434: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.0972 - accuracy: 0.9649 - val_loss: 0.0918 - val_accuracy: 0.9622
DEBUG: 2023-07-23 12:00:48,530: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.0849 - accuracy: 0.9674 - val_loss: 0.0884 - val_accuracy: 0.9635
DEBUG: 2023-07-23 12:01:09,631: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0761 - accuracy: 0.9674 - val_loss: 0.0752 - val_accuracy: 0.9696
DEBUG: 2023-07-23 12:01:30,787: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0654 - accuracy: 0.9701 - val_loss: 0.0637 - val_accuracy: 0.975
DEBUG: 2023-07-23 12:01:51,800: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.1052 - accuracy: 0.9573 - val_loss: 0.2255 - val_accuracy: 0.9325
DEBUG: 2023-07-23 12:02:12,988: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.1203 - accuracy: 0.9593 - val_loss: 0.0914 - val_accuracy: 0.9676
DEBUG: 2023-07-23 12:02:34,045: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.102 - accuracy: 0.9656 - val_loss: 0.0961 - val_accuracy: 0.9608
DEBUG: 2023-07-23 12:02:55,569: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0515 - accuracy: 0.9762 - val_loss: 0.0622 - val_accuracy: 0.9683
DEBUG: 2023-07-23 12:03:17,114: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.1197 - accuracy: 0.9561 - val_loss: 0.1182 - val_accuracy: 0.9561
DEBUG: 2023-07-23 12:03:38,492: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.052 - accuracy: 0.9759 - val_loss: 0.0781 - val_accuracy: 0.9662
DEBUG: 2023-07-23 12:04:00,046: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0423 - accuracy: 0.9828 - val_loss: 0.0423 - val_accuracy: 0.9818
DEBUG: 2023-07-23 12:04:21,561: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.0632 - accuracy: 0.9803 - val_loss: 0.1215 - val_accuracy: 0.9602
DEBUG: 2023-07-23 12:04:43,006: keras_callback.py: on_epoch_end: Epoch 130/10000 - loss: 0.0492 - accuracy: 0.9786 - val_loss: 0.0496 - val_accuracy: 0.9784
DEBUG: 2023-07-23 12:05:04,429: keras_callback.py: on_epoch_end: Epoch 140/10000 - loss: 0.0313 - accuracy: 0.986 - val_loss: 0.0467 - val_accuracy: 0.9831
DEBUG: 2023-07-23 12:09:44,792: 4266479415.py: <module>: ---Cross Validation Scores---
DEBUG: 2023-07-23 12:09:44,794: 4266479415.py: <module>: ---train---
DEBUG: 2023-07-23 12:09:44,795: 4266479415.py: <module>: logloss=0.047611
DEBUG: 2023-07-23 12:09:44,796: 4266479415.py: <module>: accuracy=0.97637
DEBUG: 2023-07-23 12:09:44,797: 4266479415.py: <module>: precision=0.979338
DEBUG: 2023-07-23 12:09:44,798: 4266479415.py: <module>: recall=0.978081
DEBUG: 2023-07-23 12:09:44,799: 4266479415.py: <module>: f1=0.978282
DEBUG: 2023-07-23 12:09:44,800: 4266479415.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.999489, 'WALKING_UPSTAIRS': 0.999651, 'WALKING_DOWNSTAIRS': 0.99962, 'SITTING': 0.930165, 'STANDING': 0.940764}
DEBUG: 2023-07-23 12:09:44,800: 4266479415.py: <module>: ---valid---
DEBUG: 2023-07-23 12:09:44,801: 4266479415.py: <module>: logloss=0.049978
DEBUG: 2023-07-23 12:09:44,802: 4266479415.py: <module>: accuracy=0.974346
DEBUG: 2023-07-23 12:09:44,802: 4266479415.py: <module>: precision=0.977387
DEBUG: 2023-07-23 12:09:44,803: 4266479415.py: <module>: recall=0.976055
DEBUG: 2023-07-23 12:09:44,804: 4266479415.py: <module>: f1=0.976339
DEBUG: 2023-07-23 12:09:44,804: 4266479415.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.997561, 'WALKING_UPSTAIRS': 0.998595, 'WALKING_DOWNSTAIRS': 0.99898, 'SITTING': 0.92602, 'STANDING': 0.936876}
DEBUG: 2023-07-23 12:09:44,805: 4266479415.py: <module>: ---test---
DEBUG: 2023-07-23 12:09:44,806: 4266479415.py: <module>: logloss=123.559282
DEBUG: 2023-07-23 12:09:44,807: 4266479415.py: <module>: accuracy=0.186368
DEBUG: 2023-07-23 12:09:44,807: 4266479415.py: <module>: precision=0.166667
DEBUG: 2023-07-23 12:09:44,808: 4266479415.py: <module>: recall=0.031061
DEBUG: 2023-07-23 12:09:44,809: 4266479415.py: <module>: f1=0.052362
DEBUG: 2023-07-23 12:09:44,810: 4266479415.py: <module>: per-class f1={'LAYING': 0.314171, 'WALKING': 0.0, 'WALKING_UPSTAIRS': 0.0, 'WALKING_DOWNSTAIRS': 0.0, 'SITTING': 0.0, 'STANDING': 0.0}
