DEBUG: 2023-07-24 15:57:27,584: 62444462.py: <module>: /home/tahoora/Projects/college_project/Human_Activity_Recognition/notebooks/stacked_lstm_logs/deep-conv-lstm-20230724-155727/deep-conv-lstm-20230724-155727.log
DEBUG: 2023-07-24 15:57:40,557: 62444462.py: <module>: X_train.shape=(7406, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 15:57:40,558: 62444462.py: <module>: y_train.shape=(7406, 1) y_test.shape=(2993, 1)
DEBUG: 2023-07-24 15:57:40,561: utils.py: check_class_balance: train labels
DEBUG: 2023-07-24 15:57:40,562: utils.py: check_class_balance: LAYING (0): 1412 samples (19.07 %)
DEBUG: 2023-07-24 15:57:40,563: utils.py: check_class_balance: WALKING (1): 1224 samples (16.53 %)
DEBUG: 2023-07-24 15:57:40,564: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 1072 samples (14.47 %)
DEBUG: 2023-07-24 15:57:40,564: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 987 samples (13.33 %)
DEBUG: 2023-07-24 15:57:40,565: utils.py: check_class_balance: SITTING (4): 1290 samples (17.42 %)
DEBUG: 2023-07-24 15:57:40,566: utils.py: check_class_balance: STANDING (5): 1421 samples (19.19 %)
DEBUG: 2023-07-24 15:57:40,568: utils.py: check_class_balance: test labels
DEBUG: 2023-07-24 15:57:40,569: utils.py: check_class_balance: LAYING (0): 545 samples (18.21 %)
DEBUG: 2023-07-24 15:57:40,570: utils.py: check_class_balance: WALKING (1): 496 samples (16.57 %)
DEBUG: 2023-07-24 15:57:40,571: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 470 samples (15.7 %)
DEBUG: 2023-07-24 15:57:40,571: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 419 samples (14.0 %)
DEBUG: 2023-07-24 15:57:40,572: utils.py: check_class_balance: SITTING (4): 507 samples (16.94 %)
DEBUG: 2023-07-24 15:57:40,573: utils.py: check_class_balance: STANDING (5): 556 samples (18.58 %)
DEBUG: 2023-07-24 15:57:40,599: 1134177668.py: <module>: dcl_params={'batch_size': 128, 'epochs': 10000, 'lr': 0.001, 'verbose': 0}
DEBUG: 2023-07-24 15:57:40,637: 2623015374.py: <module>: X_tr.shape=(5924, 128, 6, 1) X_val.shape=(1482, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 15:57:40,638: 2623015374.py: <module>: y_tr.shape=(5924, 6) y_val.shape=(1482, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 15:57:43,580: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 15:57:53,985: attrs.py: create: Creating converter from 5 to 3
DEBUG: 2023-07-24 15:58:05,070: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1711 - accuracy: 0.9416 - val_loss: 0.1252 - val_accuracy: 0.9588
DEBUG: 2023-07-24 15:58:16,158: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.128 - accuracy: 0.9492 - val_loss: 0.0991 - val_accuracy: 0.9656
DEBUG: 2023-07-24 15:58:29,224: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.1071 - accuracy: 0.9521 - val_loss: 0.0885 - val_accuracy: 0.9669
DEBUG: 2023-07-24 15:58:40,509: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1044 - accuracy: 0.9519 - val_loss: 0.084 - val_accuracy: 0.9649
DEBUG: 2023-07-24 15:58:52,100: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.095 - accuracy: 0.9561 - val_loss: 0.0807 - val_accuracy: 0.9717
DEBUG: 2023-07-24 15:59:04,354: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0913 - accuracy: 0.9575 - val_loss: 0.0694 - val_accuracy: 0.9696
DEBUG: 2023-07-24 15:59:17,305: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0948 - accuracy: 0.9573 - val_loss: 0.0619 - val_accuracy: 0.975
DEBUG: 2023-07-24 15:59:29,257: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0826 - accuracy: 0.9644 - val_loss: 0.0856 - val_accuracy: 0.9642
DEBUG: 2023-07-24 15:59:41,781: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0804 - accuracy: 0.9659 - val_loss: 0.1459 - val_accuracy: 0.9534
DEBUG: 2023-07-24 15:59:53,798: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0825 - accuracy: 0.961 - val_loss: 0.1138 - val_accuracy: 0.9669
DEBUG: 2023-07-24 15:59:59,626: attrs.py: __getitem__: Creating converter from 3 to 5
DEBUG: 2023-07-24 16:00:11,582: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:00:11,583: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 16:00:13,051: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 16:00:37,067: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.143 - accuracy: 0.949 - val_loss: 0.112 - val_accuracy: 0.9514
DEBUG: 2023-07-24 16:00:50,267: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1005 - accuracy: 0.9556 - val_loss: 0.0913 - val_accuracy: 0.9534
DEBUG: 2023-07-24 16:01:04,763: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0919 - accuracy: 0.9558 - val_loss: 0.1019 - val_accuracy: 0.9595
DEBUG: 2023-07-24 16:01:18,588: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0962 - accuracy: 0.9608 - val_loss: 0.0867 - val_accuracy: 0.9548
DEBUG: 2023-07-24 16:01:31,378: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.09 - accuracy: 0.9649 - val_loss: 0.1215 - val_accuracy: 0.9541
DEBUG: 2023-07-24 16:01:48,301: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0713 - accuracy: 0.9688 - val_loss: 0.0971 - val_accuracy: 0.946
DEBUG: 2023-07-24 16:02:02,481: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0844 - accuracy: 0.9625 - val_loss: 0.0847 - val_accuracy: 0.9561
DEBUG: 2023-07-24 16:02:15,123: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0668 - accuracy: 0.9703 - val_loss: 0.0679 - val_accuracy: 0.9676
DEBUG: 2023-07-24 16:02:27,483: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0584 - accuracy: 0.973 - val_loss: 0.057 - val_accuracy: 0.9683
DEBUG: 2023-07-24 16:02:39,371: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.1994 - accuracy: 0.9365 - val_loss: 0.1854 - val_accuracy: 0.9352
DEBUG: 2023-07-24 16:02:52,040: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0589 - accuracy: 0.974 - val_loss: 0.0657 - val_accuracy: 0.9703
DEBUG: 2023-07-24 16:03:05,900: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.073 - accuracy: 0.9684 - val_loss: 0.0848 - val_accuracy: 0.9588
DEBUG: 2023-07-24 16:03:19,510: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:03:19,510: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 16:03:20,584: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
WARNING: 2023-07-24 16:03:58,366: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
WARNING: 2023-07-24 16:04:22,664: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
WARNING: 2023-07-24 16:05:15,746: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
WARNING: 2023-07-24 16:06:16,392: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 16:07:05,647: 62444462.py: <module>: /home/tahoora/Projects/college_project/Human_Activity_Recognition/notebooks/stacked_lstm_logs/deep-conv-lstm-20230724-160705/deep-conv-lstm-20230724-160705.log
DEBUG: 2023-07-24 16:07:18,957: 62444462.py: <module>: X_train.shape=(7406, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:07:18,959: 62444462.py: <module>: y_train.shape=(7406, 1) y_test.shape=(2993, 1)
DEBUG: 2023-07-24 16:07:18,964: utils.py: check_class_balance: train labels
DEBUG: 2023-07-24 16:07:18,965: utils.py: check_class_balance: LAYING (0): 1412 samples (19.07 %)
DEBUG: 2023-07-24 16:07:18,967: utils.py: check_class_balance: WALKING (1): 1224 samples (16.53 %)
DEBUG: 2023-07-24 16:07:18,968: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 1072 samples (14.47 %)
DEBUG: 2023-07-24 16:07:18,970: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 987 samples (13.33 %)
DEBUG: 2023-07-24 16:07:18,972: utils.py: check_class_balance: SITTING (4): 1290 samples (17.42 %)
DEBUG: 2023-07-24 16:07:18,973: utils.py: check_class_balance: STANDING (5): 1421 samples (19.19 %)
DEBUG: 2023-07-24 16:07:18,975: utils.py: check_class_balance: test labels
DEBUG: 2023-07-24 16:07:18,976: utils.py: check_class_balance: LAYING (0): 545 samples (18.21 %)
DEBUG: 2023-07-24 16:07:18,978: utils.py: check_class_balance: WALKING (1): 496 samples (16.57 %)
DEBUG: 2023-07-24 16:07:18,980: utils.py: check_class_balance: WALKING_UPSTAIRS (2): 470 samples (15.7 %)
DEBUG: 2023-07-24 16:07:18,981: utils.py: check_class_balance: WALKING_DOWNSTAIRS (3): 419 samples (14.0 %)
DEBUG: 2023-07-24 16:07:18,984: utils.py: check_class_balance: SITTING (4): 507 samples (16.94 %)
DEBUG: 2023-07-24 16:07:18,985: utils.py: check_class_balance: STANDING (5): 556 samples (18.58 %)
DEBUG: 2023-07-24 16:07:19,014: 1134177668.py: <module>: dcl_params={'batch_size': 128, 'epochs': 10000, 'lr': 0.001, 'verbose': 0}
DEBUG: 2023-07-24 16:07:19,049: 2623015374.py: <module>: X_tr.shape=(5924, 128, 6, 1) X_val.shape=(1482, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:07:19,050: 2623015374.py: <module>: y_tr.shape=(5924, 6) y_val.shape=(1482, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 16:07:20,117: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 16:07:42,283: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1716 - accuracy: 0.9365 - val_loss: 0.1434 - val_accuracy: 0.944
DEBUG: 2023-07-24 16:07:54,768: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.136 - accuracy: 0.9472 - val_loss: 0.1144 - val_accuracy: 0.9615
DEBUG: 2023-07-24 16:08:08,895: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0873 - accuracy: 0.9581 - val_loss: 0.0839 - val_accuracy: 0.9642
DEBUG: 2023-07-24 16:08:21,528: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0827 - accuracy: 0.9625 - val_loss: 0.0808 - val_accuracy: 0.971
DEBUG: 2023-07-24 16:08:33,487: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.1026 - accuracy: 0.9548 - val_loss: 0.0842 - val_accuracy: 0.9737
DEBUG: 2023-07-24 16:08:47,005: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0927 - accuracy: 0.9591 - val_loss: 0.0811 - val_accuracy: 0.9609
DEBUG: 2023-07-24 16:09:01,004: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0702 - accuracy: 0.9656 - val_loss: 0.0717 - val_accuracy: 0.9744
DEBUG: 2023-07-24 16:09:20,278: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:09:20,280: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 16:09:22,216: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 16:09:45,799: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.2805 - accuracy: 0.9094 - val_loss: 0.162 - val_accuracy: 0.9473
DEBUG: 2023-07-24 16:09:58,415: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1299 - accuracy: 0.9489 - val_loss: 0.1133 - val_accuracy: 0.9507
DEBUG: 2023-07-24 16:10:10,086: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0876 - accuracy: 0.9617 - val_loss: 0.1045 - val_accuracy: 0.9568
DEBUG: 2023-07-24 16:10:21,753: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.082 - accuracy: 0.96 - val_loss: 0.0964 - val_accuracy: 0.9521
DEBUG: 2023-07-24 16:10:33,660: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.0742 - accuracy: 0.9615 - val_loss: 0.0851 - val_accuracy: 0.9541
DEBUG: 2023-07-24 16:10:45,752: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0776 - accuracy: 0.9644 - val_loss: 0.0841 - val_accuracy: 0.9615
DEBUG: 2023-07-24 16:10:57,867: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0837 - accuracy: 0.9681 - val_loss: 0.0955 - val_accuracy: 0.9527
DEBUG: 2023-07-24 16:11:09,907: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0664 - accuracy: 0.9755 - val_loss: 0.0911 - val_accuracy: 0.9642
DEBUG: 2023-07-24 16:11:21,892: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0661 - accuracy: 0.9656 - val_loss: 0.0727 - val_accuracy: 0.9602
DEBUG: 2023-07-24 16:11:34,072: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0695 - accuracy: 0.9659 - val_loss: 0.0814 - val_accuracy: 0.9541
DEBUG: 2023-07-24 16:11:47,464: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0658 - accuracy: 0.9676 - val_loss: 0.0767 - val_accuracy: 0.9615
DEBUG: 2023-07-24 16:12:06,650: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:12:06,652: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 16:12:07,896: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 16:12:33,340: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.1783 - accuracy: 0.943 - val_loss: 0.1553 - val_accuracy: 0.9419
DEBUG: 2023-07-24 16:12:47,738: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1152 - accuracy: 0.9559 - val_loss: 0.1276 - val_accuracy: 0.9507
DEBUG: 2023-07-24 16:13:00,389: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.0986 - accuracy: 0.9553 - val_loss: 0.1127 - val_accuracy: 0.9467
DEBUG: 2023-07-24 16:13:11,636: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.0855 - accuracy: 0.9602 - val_loss: 0.0974 - val_accuracy: 0.9527
DEBUG: 2023-07-24 16:13:23,408: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.1018 - accuracy: 0.9583 - val_loss: 0.1077 - val_accuracy: 0.9521
DEBUG: 2023-07-24 16:13:37,386: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0726 - accuracy: 0.9644 - val_loss: 0.0973 - val_accuracy: 0.9487
DEBUG: 2023-07-24 16:13:49,048: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.1049 - accuracy: 0.9546 - val_loss: 0.0969 - val_accuracy: 0.9507
DEBUG: 2023-07-24 16:14:02,562: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.1143 - accuracy: 0.9602 - val_loss: 0.1204 - val_accuracy: 0.9568
DEBUG: 2023-07-24 16:14:15,815: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0835 - accuracy: 0.9693 - val_loss: 0.1048 - val_accuracy: 0.9595
DEBUG: 2023-07-24 16:14:27,315: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.1042 - accuracy: 0.9522 - val_loss: 0.1181 - val_accuracy: 0.9548
DEBUG: 2023-07-24 16:14:38,798: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0562 - accuracy: 0.9767 - val_loss: 0.0808 - val_accuracy: 0.9703
DEBUG: 2023-07-24 16:14:52,790: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.0593 - accuracy: 0.9745 - val_loss: 0.0739 - val_accuracy: 0.9683
DEBUG: 2023-07-24 16:15:04,802: keras_callback.py: on_epoch_end: Epoch 130/10000 - loss: 0.0641 - accuracy: 0.97 - val_loss: 0.071 - val_accuracy: 0.9696
DEBUG: 2023-07-24 16:15:16,801: keras_callback.py: on_epoch_end: Epoch 140/10000 - loss: 0.0789 - accuracy: 0.9691 - val_loss: 0.1574 - val_accuracy: 0.95
DEBUG: 2023-07-24 16:15:28,093: keras_callback.py: on_epoch_end: Epoch 150/10000 - loss: 0.0716 - accuracy: 0.9654 - val_loss: 0.0913 - val_accuracy: 0.9494
DEBUG: 2023-07-24 16:15:39,559: keras_callback.py: on_epoch_end: Epoch 160/10000 - loss: 0.0595 - accuracy: 0.9738 - val_loss: 0.0685 - val_accuracy: 0.9696
DEBUG: 2023-07-24 16:15:51,188: keras_callback.py: on_epoch_end: Epoch 170/10000 - loss: 0.0715 - accuracy: 0.9652 - val_loss: 0.0892 - val_accuracy: 0.9568
DEBUG: 2023-07-24 16:16:02,567: keras_callback.py: on_epoch_end: Epoch 180/10000 - loss: 0.0538 - accuracy: 0.9774 - val_loss: 0.0613 - val_accuracy: 0.9716
DEBUG: 2023-07-24 16:16:13,837: keras_callback.py: on_epoch_end: Epoch 190/10000 - loss: 0.0494 - accuracy: 0.9792 - val_loss: 0.0657 - val_accuracy: 0.977
DEBUG: 2023-07-24 16:16:28,617: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:16:28,619: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 16:16:29,638: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 16:16:50,400: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.177 - accuracy: 0.9433 - val_loss: 0.1551 - val_accuracy: 0.9494
DEBUG: 2023-07-24 16:17:02,133: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1351 - accuracy: 0.9489 - val_loss: 0.1183 - val_accuracy: 0.9534
DEBUG: 2023-07-24 16:17:13,391: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.126 - accuracy: 0.9532 - val_loss: 0.0954 - val_accuracy: 0.9588
DEBUG: 2023-07-24 16:17:26,416: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1183 - accuracy: 0.9527 - val_loss: 0.1148 - val_accuracy: 0.9494
DEBUG: 2023-07-24 16:17:37,706: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.1313 - accuracy: 0.9512 - val_loss: 0.0977 - val_accuracy: 0.9541
DEBUG: 2023-07-24 16:17:49,494: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0741 - accuracy: 0.9659 - val_loss: 0.0823 - val_accuracy: 0.9615
DEBUG: 2023-07-24 16:18:00,819: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0755 - accuracy: 0.9649 - val_loss: 0.083 - val_accuracy: 0.9642
DEBUG: 2023-07-24 16:18:13,102: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0962 - accuracy: 0.9602 - val_loss: 0.1043 - val_accuracy: 0.9581
DEBUG: 2023-07-24 16:18:27,699: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0795 - accuracy: 0.9661 - val_loss: 0.0776 - val_accuracy: 0.9629
DEBUG: 2023-07-24 16:18:40,821: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0724 - accuracy: 0.9664 - val_loss: 0.0731 - val_accuracy: 0.9622
DEBUG: 2023-07-24 16:18:52,980: keras_callback.py: on_epoch_end: Epoch 110/10000 - loss: 0.0878 - accuracy: 0.9614 - val_loss: 0.0777 - val_accuracy: 0.9656
DEBUG: 2023-07-24 16:19:05,367: keras_callback.py: on_epoch_end: Epoch 120/10000 - loss: 0.0688 - accuracy: 0.9668 - val_loss: 0.073 - val_accuracy: 0.9649
DEBUG: 2023-07-24 16:19:17,708: keras_callback.py: on_epoch_end: Epoch 130/10000 - loss: 0.0733 - accuracy: 0.9661 - val_loss: 0.0679 - val_accuracy: 0.9649
DEBUG: 2023-07-24 16:19:32,003: keras_callback.py: on_epoch_end: Epoch 140/10000 - loss: 0.0574 - accuracy: 0.9722 - val_loss: 0.0718 - val_accuracy: 0.9696
DEBUG: 2023-07-24 16:19:45,229: keras_callback.py: on_epoch_end: Epoch 150/10000 - loss: 0.0483 - accuracy: 0.9782 - val_loss: 0.0552 - val_accuracy: 0.9764
DEBUG: 2023-07-24 16:19:58,623: keras_callback.py: on_epoch_end: Epoch 160/10000 - loss: 0.0582 - accuracy: 0.9749 - val_loss: 0.0683 - val_accuracy: 0.973
DEBUG: 2023-07-24 16:20:09,908: keras_callback.py: on_epoch_end: Epoch 170/10000 - loss: 0.0436 - accuracy: 0.9821 - val_loss: 0.0622 - val_accuracy: 0.975
DEBUG: 2023-07-24 16:20:22,460: keras_callback.py: on_epoch_end: Epoch 180/10000 - loss: 0.065 - accuracy: 0.9649 - val_loss: 0.0658 - val_accuracy: 0.9629
DEBUG: 2023-07-24 16:20:40,168: 2623015374.py: <module>: X_tr.shape=(5925, 128, 6, 1) X_val.shape=(1481, 128, 6, 1) X_test.shape=(2993, 128, 6, 1)
DEBUG: 2023-07-24 16:20:40,169: 2623015374.py: <module>: y_tr.shape=(5925, 6) y_val.shape=(1481, 6) y_test.shape=(2993, 6)
WARNING: 2023-07-24 16:20:41,267: optimizer.py: _process_kwargs: `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.
DEBUG: 2023-07-24 16:21:01,288: keras_callback.py: on_epoch_end: Epoch 10/10000 - loss: 0.122 - accuracy: 0.9529 - val_loss: 0.1188 - val_accuracy: 0.946
DEBUG: 2023-07-24 16:21:13,259: keras_callback.py: on_epoch_end: Epoch 20/10000 - loss: 0.1508 - accuracy: 0.9526 - val_loss: 0.131 - val_accuracy: 0.9507
DEBUG: 2023-07-24 16:21:24,846: keras_callback.py: on_epoch_end: Epoch 30/10000 - loss: 0.096 - accuracy: 0.9614 - val_loss: 0.1017 - val_accuracy: 0.9514
DEBUG: 2023-07-24 16:21:36,641: keras_callback.py: on_epoch_end: Epoch 40/10000 - loss: 0.1676 - accuracy: 0.9489 - val_loss: 0.4102 - val_accuracy: 0.8589
DEBUG: 2023-07-24 16:21:48,420: keras_callback.py: on_epoch_end: Epoch 50/10000 - loss: 0.1079 - accuracy: 0.9559 - val_loss: 0.141 - val_accuracy: 0.9575
DEBUG: 2023-07-24 16:22:00,414: keras_callback.py: on_epoch_end: Epoch 60/10000 - loss: 0.0745 - accuracy: 0.9595 - val_loss: 0.0936 - val_accuracy: 0.9622
DEBUG: 2023-07-24 16:22:12,208: keras_callback.py: on_epoch_end: Epoch 70/10000 - loss: 0.0939 - accuracy: 0.9586 - val_loss: 0.0786 - val_accuracy: 0.9548
DEBUG: 2023-07-24 16:22:25,168: keras_callback.py: on_epoch_end: Epoch 80/10000 - loss: 0.0665 - accuracy: 0.9639 - val_loss: 0.0746 - val_accuracy: 0.9581
DEBUG: 2023-07-24 16:22:38,102: keras_callback.py: on_epoch_end: Epoch 90/10000 - loss: 0.0956 - accuracy: 0.9605 - val_loss: 0.1258 - val_accuracy: 0.9541
DEBUG: 2023-07-24 16:22:50,684: keras_callback.py: on_epoch_end: Epoch 100/10000 - loss: 0.0762 - accuracy: 0.9637 - val_loss: 0.0902 - val_accuracy: 0.9588
DEBUG: 2023-07-24 16:23:15,792: 4266479415.py: <module>: ---Cross Validation Scores---
DEBUG: 2023-07-24 16:23:15,795: 4266479415.py: <module>: ---train---
DEBUG: 2023-07-24 16:23:15,800: 4266479415.py: <module>: logloss=0.055946
DEBUG: 2023-07-24 16:23:15,804: 4266479415.py: <module>: accuracy=0.971982
DEBUG: 2023-07-24 16:23:15,806: 4266479415.py: <module>: precision=0.975459
DEBUG: 2023-07-24 16:23:15,809: 4266479415.py: <module>: recall=0.973951
DEBUG: 2023-07-24 16:23:15,813: 4266479415.py: <module>: f1=0.974304
DEBUG: 2023-07-24 16:23:15,817: 4266479415.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.999388, 'WALKING_UPSTAIRS': 1.0, 'WALKING_DOWNSTAIRS': 0.999239, 'SITTING': 0.918336, 'STANDING': 0.928862}
DEBUG: 2023-07-24 16:23:15,819: 4266479415.py: <module>: ---valid---
DEBUG: 2023-07-24 16:23:15,821: 4266479415.py: <module>: logloss=0.063316
DEBUG: 2023-07-24 16:23:15,823: 4266479415.py: <module>: accuracy=0.968808
DEBUG: 2023-07-24 16:23:15,827: 4266479415.py: <module>: precision=0.972761
DEBUG: 2023-07-24 16:23:15,830: 4266479415.py: <module>: recall=0.97072
DEBUG: 2023-07-24 16:23:15,832: 4266479415.py: <module>: f1=0.971084
DEBUG: 2023-07-24 16:23:15,835: 4266479415.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.996322, 'WALKING_UPSTAIRS': 0.99768, 'WALKING_DOWNSTAIRS': 0.996456, 'SITTING': 0.911714, 'STANDING': 0.924329}
DEBUG: 2023-07-24 16:23:15,837: 4266479415.py: <module>: ---test---
DEBUG: 2023-07-24 16:23:15,839: 4266479415.py: <module>: logloss=0.383353
DEBUG: 2023-07-24 16:23:15,840: 4266479415.py: <module>: accuracy=0.934581
DEBUG: 2023-07-24 16:23:15,842: 4266479415.py: <module>: precision=0.935768
DEBUG: 2023-07-24 16:23:15,844: 4266479415.py: <module>: recall=0.936103
DEBUG: 2023-07-24 16:23:15,846: 4266479415.py: <module>: f1=0.934753
DEBUG: 2023-07-24 16:23:15,849: 4266479415.py: <module>: per-class f1={'LAYING': 0.990242, 'WALKING': 0.94274, 'WALKING_UPSTAIRS': 0.956791, 'WALKING_DOWNSTAIRS': 0.936202, 'SITTING': 0.879812, 'STANDING': 0.902732}
DEBUG: 2023-07-24 16:23:15,859: 2315415076.py: <module>: ---Final Test Scores Averaged over Folds---
DEBUG: 2023-07-24 16:27:21,916: 4266479415.py: <module>: ---Cross Validation Scores---
DEBUG: 2023-07-24 16:27:21,919: 4266479415.py: <module>: ---train---
DEBUG: 2023-07-24 16:27:21,922: 4266479415.py: <module>: logloss=0.055946
DEBUG: 2023-07-24 16:27:21,924: 4266479415.py: <module>: accuracy=0.971982
DEBUG: 2023-07-24 16:27:21,926: 4266479415.py: <module>: precision=0.975459
DEBUG: 2023-07-24 16:27:21,927: 4266479415.py: <module>: recall=0.973951
DEBUG: 2023-07-24 16:27:21,929: 4266479415.py: <module>: f1=0.974304
DEBUG: 2023-07-24 16:27:21,930: 4266479415.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.999388, 'WALKING_UPSTAIRS': 1.0, 'WALKING_DOWNSTAIRS': 0.999239, 'SITTING': 0.918336, 'STANDING': 0.928862}
DEBUG: 2023-07-24 16:27:21,932: 4266479415.py: <module>: ---valid---
DEBUG: 2023-07-24 16:27:21,933: 4266479415.py: <module>: logloss=0.063316
DEBUG: 2023-07-24 16:27:21,935: 4266479415.py: <module>: accuracy=0.968808
DEBUG: 2023-07-24 16:27:21,936: 4266479415.py: <module>: precision=0.972761
DEBUG: 2023-07-24 16:27:21,939: 4266479415.py: <module>: recall=0.97072
DEBUG: 2023-07-24 16:27:21,941: 4266479415.py: <module>: f1=0.971084
DEBUG: 2023-07-24 16:27:21,942: 4266479415.py: <module>: per-class f1={'LAYING': 1.0, 'WALKING': 0.996322, 'WALKING_UPSTAIRS': 0.99768, 'WALKING_DOWNSTAIRS': 0.996456, 'SITTING': 0.911714, 'STANDING': 0.924329}
DEBUG: 2023-07-24 16:27:21,944: 4266479415.py: <module>: ---test---
DEBUG: 2023-07-24 16:27:21,947: 4266479415.py: <module>: logloss=0.383353
DEBUG: 2023-07-24 16:27:21,951: 4266479415.py: <module>: accuracy=0.934581
DEBUG: 2023-07-24 16:27:21,952: 4266479415.py: <module>: precision=0.935768
DEBUG: 2023-07-24 16:27:21,956: 4266479415.py: <module>: recall=0.936103
DEBUG: 2023-07-24 16:27:21,958: 4266479415.py: <module>: f1=0.934753
DEBUG: 2023-07-24 16:27:21,959: 4266479415.py: <module>: per-class f1={'LAYING': 0.990242, 'WALKING': 0.94274, 'WALKING_UPSTAIRS': 0.956791, 'WALKING_DOWNSTAIRS': 0.936202, 'SITTING': 0.879812, 'STANDING': 0.902732}
DEBUG: 2023-07-24 16:27:43,960: 2315415076.py: <module>: ---Final Test Scores Averaged over Folds---
